---
title: '量子位发表了文章: 谷歌爆改Transformer，“无限注意力”让1B小模型读完10部小说，114倍信息压缩'
date: '2024-04-12'
linkTitle: https://zhuanlan.zhihu.com/p/692209307
source: 量子位的知乎动态
description: <blockquote data-pid="nPWoZb1R">明敏 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p
  data-pid="N_wdZWXG">谷歌大改Transformer，“无限”长度上下文来了。</p><p data-pid="GywuhrGd">现在，<b>1B大模型上下文长度可扩展到1M</b>（100万token，大约相当于10部小说），并能完成Passkey检索任务。</p><p
  data-pid="5kjCIGI7"><b>8B大模型在500K上下文长度</b>的书籍摘要任务中，拿下最<b>新SOTA</b>。</p><p data-pid="1VrIG8XS">这就是谷歌最新提出的<b>Infini-attention机制</b>（无限注意力）。</p><p
  class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-170badd525eda463f73a0ca0e4558a21_1440w.jpg"
  data-caption=""  ...
disable_comments: true
---
<blockquote data-pid="nPWoZb1R">明敏 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p data-pid="N_wdZWXG">谷歌大改Transformer，“无限”长度上下文来了。</p><p data-pid="GywuhrGd">现在，<b>1B大模型上下文长度可扩展到1M</b>（100万token，大约相当于10部小说），并能完成Passkey检索任务。</p><p data-pid="5kjCIGI7"><b>8B大模型在500K上下文长度</b>的书籍摘要任务中，拿下最<b>新SOTA</b>。</p><p data-pid="1VrIG8XS">这就是谷歌最新提出的<b>Infini-attention机制</b>（无限注意力）。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-170badd525eda463f73a0ca0e4558a21_1440w.jpg" data-caption=""  ...