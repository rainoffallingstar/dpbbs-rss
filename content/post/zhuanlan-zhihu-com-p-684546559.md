---
title: '量子位发表了文章: 陈丹琦团队新作：Llama-2上下文扩展至128k，10倍吞吐量仅需1/6内存'
date: '2024-02-29'
linkTitle: https://zhuanlan.zhihu.com/p/684546559
source: 量子位的知乎动态
description: <blockquote data-pid="Jmelxmtn">丰色 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p
  data-pid="e2hEV9Ls">陈丹琦团队刚刚发布了一种新的LLM<b>上下文窗口扩展</b>方法：</p><p data-pid="ATOZasSb">它仅用8k大小的token文档进行训练，就能将Llama-2窗口扩展至128k。</p><p
  data-pid="6ND5l7H2">最重要的是，在这个过程中，只需要原来<b>1/6的内存</b>，模型就获得了<b>10倍吞吐量</b>。</p><p class="ztext-empty-paragraph"><br></p><figure
  data-size="normal"><img src="https://pic3.zhimg.com/v2-2ad13c72543750a008d2ca865d401796_1440w.jpg"
  data-caption="" data-size="normal" data-rawwidth="948" data-rawheight="1100" ...
disable_comments: true
---
<blockquote data-pid="Jmelxmtn">丰色 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p data-pid="e2hEV9Ls">陈丹琦团队刚刚发布了一种新的LLM<b>上下文窗口扩展</b>方法：</p><p data-pid="ATOZasSb">它仅用8k大小的token文档进行训练，就能将Llama-2窗口扩展至128k。</p><p data-pid="6ND5l7H2">最重要的是，在这个过程中，只需要原来<b>1/6的内存</b>，模型就获得了<b>10倍吞吐量</b>。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-2ad13c72543750a008d2ca865d401796_1440w.jpg" data-caption="" data-size="normal" data-rawwidth="948" data-rawheight="1100" ...