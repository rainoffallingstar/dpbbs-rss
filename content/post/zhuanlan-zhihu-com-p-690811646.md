---
title: '量子位发表了文章: 谷歌更新Transformer架构，更节省计算资源！50%性能提升'
date: '2024-04-05'
linkTitle: https://zhuanlan.zhihu.com/p/690811646
source: 量子位的知乎动态
description: <blockquote data-pid="6kwGJNtD">明敏 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p
  data-pid="KTMGnoym">谷歌终于更新了Transformer架构。</p><p data-pid="8xI4ANTx">最新发布的<b>Mixture-of-Depths</b>（MoD），改变了以往Transformer计算模式。</p><p
  data-pid="Edcr7w9E">它通过<b>动态分配</b>大模型中的计算资源，跳过一些不必要计算，显著提高训练效率和推理速度。</p><p data-pid="dzZCL3Sz">结果显示，在等效计算量和训练时间上，MoD每次向前传播所需的计算量更小，而且后训练采样过程中步进速度<b>提高50%</b>。</p><p
  data-pid="fjVwHJGe">这一方法刚刚发布，就马上引发关注。</p><p data-pid="SaIuX-vW">MoE风头正盛，MoD已经来后浪拍前浪了？</p><p
  class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img ...
disable_comments: true
---
<blockquote data-pid="6kwGJNtD">明敏 发自 凹非寺<br>量子位 | 公众号 QbitAI</blockquote><p data-pid="KTMGnoym">谷歌终于更新了Transformer架构。</p><p data-pid="8xI4ANTx">最新发布的<b>Mixture-of-Depths</b>（MoD），改变了以往Transformer计算模式。</p><p data-pid="Edcr7w9E">它通过<b>动态分配</b>大模型中的计算资源，跳过一些不必要计算，显著提高训练效率和推理速度。</p><p data-pid="dzZCL3Sz">结果显示，在等效计算量和训练时间上，MoD每次向前传播所需的计算量更小，而且后训练采样过程中步进速度<b>提高50%</b>。</p><p data-pid="fjVwHJGe">这一方法刚刚发布，就马上引发关注。</p><p data-pid="SaIuX-vW">MoE风头正盛，MoD已经来后浪拍前浪了？</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img ...