---
title: 第二十课：MoE
date: '2024-02-26'
linkTitle: https://www.bilibili.com/video/BV1jH4y177DL
source: MindSpore官方 的 bilibili 空间
description: 2023年末，Mistral发布了激动人心的大模型：Mixtral 8x7b，该模型把开放大模型的性能带到了一个新的高度，并在许多基准测试上表现优于GPT3.5。Mixtral模型把MOE（mixture
  of experts）结构的稀疏大模型再次带到主流大模型的视野中，那么MOE结构是什么样的？相较于传统Transformer稠密结构有哪些优势？本节公开课将带领大家全面学习MOE的相关内容，并使用昇思MindSpore进行演示。<br><br><iframe
  src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=1051050778&amp;high_quality=1&amp;autoplay=0"
  width="650" height="477" scrolling="no" border="0" frameborder="no" framespacing="0"
  allowfullscreen="true" referrerpolicy="no-referrer"></iframe ...
disable_comments: true
---
2023年末，Mistral发布了激动人心的大模型：Mixtral 8x7b，该模型把开放大模型的性能带到了一个新的高度，并在许多基准测试上表现优于GPT3.5。Mixtral模型把MOE（mixture of experts）结构的稀疏大模型再次带到主流大模型的视野中，那么MOE结构是什么样的？相较于传统Transformer稠密结构有哪些优势？本节公开课将带领大家全面学习MOE的相关内容，并使用昇思MindSpore进行演示。<br><br><iframe src="https://www.bilibili.com/blackboard/html5mobileplayer.html?aid=1051050778&amp;high_quality=1&amp;autoplay=0" width="650" height="477" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" referrerpolicy="no-referrer"></iframe ...